{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyP70zwXctqVzm17d66El3An"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium","widgets":{"application/vnd.jupyter.widget-state+json":{"e34423e73163452c859a878c6484f573":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cfb55a9841de413bb1520aad72c494f2","IPY_MODEL_a370f06f858a441999a85facc5a6bb26","IPY_MODEL_f38c6b56abba411ba790203cabf9b906"],"layout":"IPY_MODEL_7c5bf5cc178c44a8b56f14e3fcb3cb01"}},"cfb55a9841de413bb1520aad72c494f2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6875447f336d402e9b1cf0226265913b","placeholder":"​","style":"IPY_MODEL_f0d6360d0bcf443e848a810ee5aa4935","value":"Loading checkpoint shards: 100%"}},"a370f06f858a441999a85facc5a6bb26":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f377edb2183148c98c51ec25e6ecf353","max":33,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5181a8b4add04ef9a6d4034c3a9c13a5","value":33}},"f38c6b56abba411ba790203cabf9b906":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f78a445667e41349bad799c78943b32","placeholder":"​","style":"IPY_MODEL_4bdd9fdbae714fdb93290b796bf15b0f","value":" 33/33 [04:13&lt;00:00,  8.33s/it]"}},"7c5bf5cc178c44a8b56f14e3fcb3cb01":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6875447f336d402e9b1cf0226265913b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f0d6360d0bcf443e848a810ee5aa4935":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f377edb2183148c98c51ec25e6ecf353":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5181a8b4add04ef9a6d4034c3a9c13a5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1f78a445667e41349bad799c78943b32":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4bdd9fdbae714fdb93290b796bf15b0f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LZFxJn2qMrsy","executionInfo":{"status":"ok","timestamp":1680943891085,"user_tz":-480,"elapsed":31756,"user":{"displayName":"yuncong li","userId":"08224369488617466670"}},"outputId":"e870fc73-92ad-432a-cc45-34536066ae31"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install -r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8PHZdOAzgeJj","executionInfo":{"status":"ok","timestamp":1680944080667,"user_tz":-480,"elapsed":70587,"user":{"displayName":"yuncong li","userId":"08224369488617466670"}},"outputId":"8a614d0b-94a5-4f4f-b634-176df451bc62"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/huggingface/peft.git (from -r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 9))\n","  Cloning https://github.com/huggingface/peft.git to /tmp/pip-req-build-7o4jryzp\n","  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-req-build-7o4jryzp\n","  Resolved https://github.com/huggingface/peft.git to commit 1117d4772109a098787ce7fc297cb6cd641de6eb\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting git+https://github.com/huggingface/transformers.git (from -r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 10))\n","  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-dc4e_e2i\n","  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-dc4e_e2i\n","  Resolved https://github.com/huggingface/transformers.git to commit 656e869a4523f6a0ce90b3aacbb05cc8fb5794bb\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting accelerate\n","  Downloading accelerate-0.18.0-py3-none-any.whl (215 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 KB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: appdirs in /usr/local/lib/python3.9/dist-packages (from -r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 2)) (1.4.4)\n","Collecting loralib\n","  Downloading loralib-0.1.1-py3-none-any.whl (8.8 kB)\n","Collecting bitsandbytes\n","  Downloading bitsandbytes-0.37.2-py3-none-any.whl (84.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.2/84.2 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting black\n","  Downloading black-23.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting datasets\n","  Downloading datasets-2.11.0-py3-none-any.whl (468 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 KB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fire\n","  Downloading fire-0.5.0.tar.gz (88 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 KB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting gradio\n","  Downloading gradio-3.24.1-py3-none-any.whl (15.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from accelerate->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 1)) (5.9.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from accelerate->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 1)) (1.22.4)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from accelerate->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 1)) (6.0)\n","Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from accelerate->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 1)) (2.0.0+cu118)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from accelerate->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 1)) (23.0)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from black->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 5)) (8.1.3)\n","Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.9/dist-packages (from black->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 5)) (4.5.0)\n","Collecting pathspec>=0.9.0\n","  Downloading pathspec-0.11.1-py3-none-any.whl (29 kB)\n","Collecting mypy-extensions>=0.4.3\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from black->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 5)) (2.0.1)\n","Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.9/dist-packages (from black->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 5)) (3.2.0)\n","Requirement already satisfied: ipython>=7.8.0 in /usr/local/lib/python3.9/dist-packages (from black->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 5)) (7.34.0)\n","Collecting tokenize-rt>=3.2.0\n","  Downloading tokenize_rt-5.0.0-py2.py3-none-any.whl (5.8 kB)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 7)) (2023.3.0)\n","Collecting multiprocess\n","  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 KB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 7)) (2.27.1)\n","Collecting dill<0.3.7,>=0.3.0\n","  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 KB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 7)) (9.0.0)\n","Collecting aiohttp\n","  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting xxhash\n","  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 KB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.11.0\n","  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 KB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 7)) (1.4.4)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 7)) (4.65.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from fire->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 8)) (1.16.0)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.9/dist-packages (from fire->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 8)) (2.2.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m99.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 10)) (2022.10.31)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 10)) (3.10.7)\n","Collecting python-multipart\n","  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 KB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting websockets>=10.0\n","  Downloading websockets-11.0.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.5/129.5 KB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting uvicorn\n","  Downloading uvicorn-0.21.1-py3-none-any.whl (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 KB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from gradio->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 12)) (8.4.0)\n","Collecting httpx\n","  Downloading httpx-0.23.3-py3-none-any.whl (71 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 KB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from gradio->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 12)) (3.7.1)\n","Collecting gradio-client>=0.0.5\n","  Downloading gradio_client-0.0.8-py3-none-any.whl (20 kB)\n","Collecting fastapi\n","  Downloading fastapi-0.95.0-py3-none-any.whl (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 KB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from gradio->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 12)) (3.1.2)\n","Requirement already satisfied: altair>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from gradio->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 12)) (4.2.2)\n","Collecting mdit-py-plugins<=0.3.3\n","  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 KB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting semantic-version\n","  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from gradio->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 12)) (2.2.0)\n","Collecting aiofiles\n","  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n","Collecting ffmpy\n","  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting pydub\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Collecting orjson\n","  Downloading orjson-3.8.9-cp39-cp39-manylinux_2_28_x86_64.whl (144 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.1/144.1 KB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: markupsafe in /usr/local/lib/python3.9/dist-packages (from gradio->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 12)) (2.1.2)\n","Requirement already satisfied: pydantic in /usr/local/lib/python3.9/dist-packages (from gradio->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 12)) (1.10.7)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.9/dist-packages (from altair>=4.2.0->gradio->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 12)) (0.4)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.9/dist-packages (from altair>=4.2.0->gradio->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 12)) (0.12.0)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.9/dist-packages (from altair>=4.2.0->gradio->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 12)) (4.3.3)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 7)) (2.0.12)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting yarl<2.0,>=1.0\n","  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 7)) (22.2.0)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.9/dist-packages (from ipython>=7.8.0->black->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 5)) (2.14.0)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.9/dist-packages (from ipython>=7.8.0->black->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 5)) (5.7.1)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.9/dist-packages (from ipython>=7.8.0->black->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 5)) (0.1.6)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.9/dist-packages (from ipython>=7.8.0->black->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 5)) (4.8.0)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.9/dist-packages (from ipython>=7.8.0->black->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 5)) (67.6.1)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.9/dist-packages (from ipython>=7.8.0->black->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 5)) (0.7.5)\n","Collecting jedi>=0.16\n","  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from ipython>=7.8.0->black->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 5)) (3.0.38)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.9/dist-packages (from ipython>=7.8.0->black->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 5)) (0.2.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from ipython>=7.8.0->black->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 5)) (4.4.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.9/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 12)) (0.1.2)\n","Collecting linkify-it-py<3,>=1\n","  Downloading linkify_it_py-2.0.0-py3-none-any.whl (19 kB)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 7)) (2022.7.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 7)) (2.8.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 7)) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 7)) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 7)) (2022.12.7)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 1)) (3.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 1)) (1.11.1)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 1)) (2.0.0)\n","Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.4.0->accelerate->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 1)) (16.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.4.0->accelerate->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 1)) (3.25.2)\n","Collecting starlette<0.27.0,>=0.26.1\n","  Downloading starlette-0.26.1-py3-none-any.whl (66 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 KB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting httpcore<0.17.0,>=0.15.0\n","  Downloading httpcore-0.16.3-py3-none-any.whl (69 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.6/69.6 KB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting rfc3986[idna2008]<2,>=1.3\n","  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.9/dist-packages (from httpx->gradio->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 12)) (1.3.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 12)) (0.11.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 12)) (1.0.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 12)) (1.4.4)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 12)) (4.39.3)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 12)) (3.0.9)\n","Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 12)) (5.12.0)\n","Collecting h11>=0.8\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 KB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.9/dist-packages (from httpcore<0.17.0,>=0.15.0->httpx->gradio->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 12)) (3.6.2)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->gradio->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 12)) (3.15.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from jedi>=0.16->ipython>=7.8.0->black->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 5)) (0.8.3)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 12)) (0.19.3)\n","Collecting uc-micro-py\n","  Downloading uc_micro_py-1.0.1-py3-none-any.whl (6.2 kB)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.9/dist-packages (from pexpect>4.3->ipython>=7.8.0->black->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 5)) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.8.0->black->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 5)) (0.2.6)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.4.0->accelerate->-r /content/drive/MyDrive/projects/try-large-models/nlp/try_llama/requirements.txt (line 1)) (1.3.0)\n","Building wheels for collected packages: fire, peft, transformers, ffmpy\n","  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116952 sha256=3f7f5d819f6a750d233ad85cbe64be3c1b8804c1709010014213b3f1ed7bfd91\n","  Stored in directory: /root/.cache/pip/wheels/f7/f1/89/b9ea2bf8f80ec027a88fef1d354b3816b4d3d29530988972f6\n","  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for peft: filename=peft-0.3.0.dev0-py3-none-any.whl size=49855 sha256=3596cde19c76378a680ebe655e5c77ce4a375f80d426907f1373382cba5f9449\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-w3keup6o/wheels/2d/60/1b/0edd9dc0f0c489738b1166bc1b0b560ee368f7721f89d06e3a\n","  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-4.28.0.dev0-py3-none-any.whl size=6895957 sha256=856da09859c4a05625260fbcd1abc405024d51d627fc3054eb5208f84e214bf8\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-w3keup6o/wheels/f7/92/8c/752ff3bfcd3439805d8bbf641614da38ef3226e127ebea86ee\n","  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4707 sha256=c305a13f27e820fe43070b54241e2e1c2143da85bfdcdcc544ae1040230151ed\n","  Stored in directory: /root/.cache/pip/wheels/91/e2/96/f676aa08bfd789328c6576cd0f1fde4a3d686703bb0c247697\n","Successfully built fire peft transformers ffmpy\n","Installing collected packages: tokenizers, sentencepiece, rfc3986, pydub, ffmpy, bitsandbytes, xxhash, websockets, uc-micro-py, tokenize-rt, semantic-version, python-multipart, pathspec, orjson, mypy-extensions, multidict, loralib, jedi, h11, frozenlist, fire, dill, async-timeout, aiofiles, yarl, uvicorn, starlette, responses, multiprocess, mdit-py-plugins, linkify-it-py, huggingface-hub, httpcore, black, aiosignal, transformers, httpx, gradio-client, fastapi, aiohttp, gradio, datasets, accelerate, peft\n","Successfully installed accelerate-0.18.0 aiofiles-23.1.0 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 bitsandbytes-0.37.2 black-23.3.0 datasets-2.11.0 dill-0.3.6 fastapi-0.95.0 ffmpy-0.3.0 fire-0.5.0 frozenlist-1.3.3 gradio-3.24.1 gradio-client-0.0.8 h11-0.14.0 httpcore-0.16.3 httpx-0.23.3 huggingface-hub-0.13.4 jedi-0.18.2 linkify-it-py-2.0.0 loralib-0.1.1 mdit-py-plugins-0.3.3 multidict-6.0.4 multiprocess-0.70.14 mypy-extensions-1.0.0 orjson-3.8.9 pathspec-0.11.1 peft-0.3.0.dev0 pydub-0.25.1 python-multipart-0.0.6 responses-0.18.0 rfc3986-1.5.0 semantic-version-2.10.0 sentencepiece-0.1.97 starlette-0.26.1 tokenize-rt-5.0.0 tokenizers-0.13.3 transformers-4.28.0.dev0 uc-micro-py-1.0.1 uvicorn-0.21.1 websockets-11.0.1 xxhash-3.2.0 yarl-1.8.2\n"]}]},{"cell_type":"code","source":["import os\n","import sys\n","\n","import fire\n","import gradio as gr\n","import torch\n","import transformers\n","from peft import PeftModel\n","from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O23DCI47kEEQ","executionInfo":{"status":"ok","timestamp":1680944190152,"user_tz":-480,"elapsed":8002,"user":{"displayName":"yuncong li","userId":"08224369488617466670"}},"outputId":"9f3bb37d-7396-4dd2-e9d5-b044b7713782"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","===================================BUG REPORT===================================\n","Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n","================================================================================\n","CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n","CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n","CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n","CUDA SETUP: Detected CUDA version 118\n","CUDA SETUP: Loading binary /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /usr/lib64-nvidia did not contain libcudart.so as expected! Searching further paths...\n","  warn(msg)\n","/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n","  warn(msg)\n","/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//172.28.0.1'), PosixPath('http'), PosixPath('8013')}\n","  warn(msg)\n","/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('--listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https'), PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-a100-s-39r7oojxkpl7d --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true')}\n","  warn(msg)\n","/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n","  warn(msg)\n","/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//ipykernel.pylab.backend_inline'), PosixPath('module')}\n","  warn(msg)\n"]}]},{"cell_type":"code","source":["if torch.cuda.is_available():\n","    device = \"cuda\"\n","else:\n","    device = \"cpu\""],"metadata":{"id":"5fWpJDgqk7Oy","executionInfo":{"status":"ok","timestamp":1680944200661,"user_tz":-480,"elapsed":2,"user":{"displayName":"yuncong li","userId":"08224369488617466670"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["load_8bit: bool = False\n","base_model: str = \"/content/drive/MyDrive/data/large_models/huggingface/hub/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348\"\n","tokenizer = LlamaTokenizer.from_pretrained(base_model)\n","model = LlamaForCausalLM.from_pretrained(\n","        base_model,\n","        load_in_8bit=load_8bit,\n","        torch_dtype=torch.float16,\n","        device_map=\"auto\",\n","    )\n","\n","if not load_8bit:\n","    model.half()  # seems to fix bugs for some users.\n","\n","model.eval()\n","if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n","    model = torch.compile(model)"],"metadata":{"id":"XRd12Rzvowam","colab":{"base_uri":"https://localhost:8080/","height":151,"referenced_widgets":["e34423e73163452c859a878c6484f573","cfb55a9841de413bb1520aad72c494f2","a370f06f858a441999a85facc5a6bb26","f38c6b56abba411ba790203cabf9b906","7c5bf5cc178c44a8b56f14e3fcb3cb01","6875447f336d402e9b1cf0226265913b","f0d6360d0bcf443e848a810ee5aa4935","f377edb2183148c98c51ec25e6ecf353","5181a8b4add04ef9a6d4034c3a9c13a5","1f78a445667e41349bad799c78943b32","4bdd9fdbae714fdb93290b796bf15b0f"]},"executionInfo":{"status":"ok","timestamp":1680944616156,"user_tz":-480,"elapsed":266532,"user":{"displayName":"yuncong li","userId":"08224369488617466670"}},"outputId":"aa0fc1f6-ced9-461e-8ba3-353c0ef3bca0"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n","The class this function is called from is 'LlamaTokenizer'.\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e34423e73163452c859a878c6484f573"}},"metadata":{}}]},{"cell_type":"code","source":["def evaluate(\n","    instruction,\n","    input=None,\n","    max_new_tokens=128,\n","    num_beams=1,\n","    do_sample=False,\n","    temperature=0.1,\n","    top_p=0.75,\n","    top_k=40,\n","    **kwargs,\n","):\n","    print('do_sample: %s' % str(do_sample))\n","    prompt = instruction\n","    inputs = tokenizer(prompt, return_tensors=\"pt\")\n","    input_ids = inputs[\"input_ids\"].to(device)\n","    generation_config = GenerationConfig(\n","        max_new_tokens=max_new_tokens,\n","        num_beams=num_beams,\n","        do_sample=do_sample,\n","        temperature=temperature,\n","        # top_k=top_k,\n","        top_p=top_p,\n","        **kwargs,\n","    )\n","    with torch.no_grad():\n","        generation_output = model.generate(\n","            input_ids=input_ids,\n","            generation_config=generation_config,\n","            return_dict_in_generate=True,\n","            output_scores=True,\n","        )\n","    s = generation_output.sequences[0]\n","    output = tokenizer.decode(s)\n","    return output"],"metadata":{"id":"zGRConLtlVne","executionInfo":{"status":"ok","timestamp":1680947269258,"user_tz":-480,"elapsed":462,"user":{"displayName":"yuncong li","userId":"08224369488617466670"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["instruction = 'write a story'\n","response = evaluate(\n","    instruction,\n","    max_new_tokens=128,\n","    num_beams=1,\n","    do_sample=True,\n","    temperature=2.0,\n","    top_p=0.9,\n","    top_k=40,\n","    )\n","print(response)"],"metadata":{"id":"--b6mWSEocpl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680947282423,"user_tz":-480,"elapsed":8499,"user":{"displayName":"yuncong li","userId":"08224369488617466670"}},"outputId":"844d7af9-2e0f-4a07-b4a7-5a135c0d205a"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["do_sample: True\n"," ⁇  write a story around? Write 60 or even less words in each chapter/scene? Or should that even a “recommendation’.\n","Joshua’s thoughts: As you develop each chapter it can make perfect sense just where you started, maybe where you finish makes a lot of sence… And all sorts other logical place to work in as things move a bit here forward. Some ideas though just happen for various circumstances. Sometimes with a whole thought like I see the last scene from above I want, what should the preceding scenes say? It might turn into: he has left work to drive home. That first driving scene\n"]}]},{"cell_type":"code","source":["server_name: str = \"0.0.0.0\"  # Allows to listen on all interfaces by providing '0.\n","share_gradio: bool = True\n","gr.Interface(\n","    fn=evaluate,\n","    inputs=[\n","        gr.components.Textbox(\n","            lines=2,\n","            label=\"Instruction\",\n","            placeholder=\"Tell me about LLaMA.\",\n","        ),\n","        gr.components.Textbox(lines=2, label=\"Input\", placeholder=\"none\"),\n","        gr.components.Slider(\n","            minimum=1, maximum=2000, step=1, value=128, label=\"Max tokens\"\n","        ),\n","        gr.components.Slider(\n","            minimum=1, maximum=4, step=1, value=4, label=\"Beams\"\n","        ),\n","        gr.components.Checkbox(value=False, label='Do sample'),\n","        gr.components.Slider(\n","            minimum=0, maximum=25, value=0.1, label=\"Temperature\"\n","        ),\n","        gr.components.Slider(\n","            minimum=0, maximum=1, value=0.75, label=\"Top p\"\n","        ),\n","        gr.components.Slider(\n","            minimum=0, maximum=100, step=1, value=40, label=\"Top k\"\n","        ),\n","    ],\n","    outputs=[\n","        gr.inputs.Textbox(\n","            lines=5,\n","            label=\"Output\",\n","        )\n","    ],\n","    title=\"LLaMA\",\n","    description=\"LLaMA.\",\n",").launch(server_name=server_name, share=share_gradio)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":719},"id":"G9erWoVZOW9j","executionInfo":{"status":"ok","timestamp":1680947291406,"user_tz":-480,"elapsed":3761,"user":{"displayName":"yuncong li","userId":"08224369488617466670"}},"outputId":"e01db307-b586-4cdc-9411-4c28b8a25487"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/gradio/inputs.py:27: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/gradio/deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect\n","  warnings.warn(value)\n","/usr/local/lib/python3.9/dist-packages/gradio/deprecation.py:40: UserWarning: `numeric` parameter is deprecated, and it has no effect\n","  warnings.warn(value)\n"]},{"output_type":"stream","name":"stdout","text":["Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","Running on public URL: https://6dd40435eb753cd937.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://6dd40435eb753cd937.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["gr.close_all()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mz2ZkAb0PML3","executionInfo":{"status":"ok","timestamp":1680947474781,"user_tz":-480,"elapsed":472,"user":{"displayName":"yuncong li","userId":"08224369488617466670"}},"outputId":"15972ca4-e6f2-4126-d881-a657c05a7c1f"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["Closing server running on port: 7860\n","Closing server running on port: 7860\n","Closing server running on port: 7860\n"]}]}]}